# 2026.01.05 开发日志

## 阶段二
 * `目标` - 能够计算一次会话对上下文的占用情况
  - 1. 整合工具，计算每次对话对上下文的占用情况
 * `步骤`
  - 2. 需要 token 计数工具来检查文本是否超过模型上下文
 * `问题` -
  - 1. 我如何知道我的一次会话是否超过大模型的上下文呢？
      * `答案`：可以通过以下方法检查：
        - **方法1：使用 `count_tokens()` 方法** - 在发送请求前计算 messages 的 token 数量
          ```python
          client = LLMClient(platform="alibaba")
          messages = [{"role": "user", "content": "你好"}]
          token_count = client.count_tokens(messages)
          print(f"Token count: {token_count}")
          ```
        - **方法2：使用 `check_context_limit()` 方法** - 检查是否超过上下文限制（推荐）
          ```python
          is_within_limit, info = client.check_context_limit(messages, max_tokens=1000)
          print(f"Input tokens: {info['input_tokens']}")
          print(f"Context limit: {info['context_limit']}")
          print(f"Usage: {info['usage_percentage']:.1f}%")
          print(f"Within limit: {is_within_limit}")
          ```
        - **方法3：自动检查** - `chat()` 方法默认会检查上下文限制，如果接近或超过会发出警告
          ```python
          # 默认 check_context=True，会自动检查并警告
          response = client.chat(messages=messages)
          ```
        - **常见模型的上下文窗口限制**：
          - GPT-3.5-turbo: 16,385 tokens
          - GPT-4: 8,192 tokens
          - GPT-4-turbo: 128,000 tokens
          - Qwen-turbo: 8,000 tokens
          - Qwen-plus: 32,000 tokens
        - **注意事项**：
          - Token 计数需要考虑输入消息 + 预计输出 tokens
          - 建议保留 20% 的安全余量，避免实际输出超过限制
          - 如果超过限制，需要实现上下文窗口管理策略（截断、摘要等）