# 代码检查流程

## 全量测试结果呈现的问题
  重要程度从上到下排列
  1. 会对部分代码规则产出错误的结果
     - 结果
       - 导入部分功能检查结果不正确
     - 问题的原因
       - 我觉得是因为输入的`上下文太大`且`联系太多`了
       - 我猜测是模型经验足够丰富，但是模型的`脑容量`太小
     - 解决思路
       - `拆分`代码检查任务，不要求模型一次对整个文件进行分析
         - 查看模型在`多大的规模`情况下，表现最好?
         - 对输入样本进行`切割`，切割成有意义`代码片段`
       - 将`规则`变成索引，让模型一次了解相关的索引
       - 针对`代码片段`，让模型检索相关的`规则`
       - 添加`记忆机制`，让模型根据`代码片段`特征调取`规则`实际内容
       - 将分析结果保存在`外部`，在需要时再调入
     - idea 
       - 如果说`llm`都是按照`世界模型`的目标训练的话，那模型应该会把
         更多的权重用来学习什么是正确的，也许这是可以利用的地方

  2. 预期模型可以对一个函数的结构进行规范检查
     - 结果
       - 效果不太好
     - 问题的原因
       - `函数的结构` 这一特征不好定义？且状态空间太多？
     - 解决思路
       - x

  3. 一次全量检查的结果格式不固定
     - 结果
       - 每次结果格式的不一致
     - 问题的原因
       - 未使用提示词进行规范性限制
       - 未添加示例
     - 解决思路
       - 使用提示词进行文本格式限制
       - 由程序工具处理输出结果

  4. 检查结果直接文本输出，视觉效果不佳
     - 结果
       - 效果不佳
     - 问题的原因
       - xxx
     - 解决思路
       - 不解决，非问题

  5. 全量测试成本过高
     - 结果
       - 一次测试一个文件，消耗 `token` 1W左右，模型 `qwen-turbo`
     - 问题的原因
       - xxx
     - 解决思路
       - 暂不解决，可以靠自己部署模型？？

## 代码检查的流程
  + 前置动作
    + 根据预先定义的规则拆分文件为代码块
    + 总结规则所针对的问题
  + 