# 2026.01.04 开发日志

## 阶段一
 * `目标`- 走通LLM调用逻辑
 * `实验环境`-
    - Windows
    - Python 3.14.x
 * `步骤`-
    - 1. 添加LLM SDK依赖(Qwen)
        ```txt
        # HTTP client libraries for API calls
        requests>=2.31.0
        httpx>=0.24.0

        # Alibaba Cloud Tongyi Qianwen
        dashscope>=1.14.0
        ```
    - 2. 
 * `问题`-
    - 1. 当使用 API 进行请求时，是一次请求是一个单独的会话？还是一次请求开启一次会话？
      * `答案`：**一次请求就是一个单独的会话**。LLM API（包括 OpenAI 和 Alibaba Cloud）都是**无状态（stateless）**的。
        这意味着：
        - 每次调用 `chat()` 方法时，都需要将完整的对话历史（messages）一起发送
        - API 服务器不会在两次请求之间保持任何状态或记忆
    - 2. LLM 的上下文限制是针对一次请求？还是针对的一次会话呢？
      * `答案`：**上下文限制是针对一次请求**的。具体来说：
        - 上下文窗口限制（context window）是指**单次 API 请求**中，输入消息（messages）和输出响应的总 token 数
        - 例如：GPT-3.5-turbo 的上下文窗口是 16K tokens，这意味着你单次请求的 messages 列表 + 生成的回复总 token 数不能超过 16K
        - 由于每次请求都是独立的，不存在跨请求的"会话级"上下文限制
        - 但需要注意的是，如果维护多轮对话，每次请求都要包含所有历史消息，所以实际上多轮对话的上下文限制就是单次请求的限制
        - 如果对话历史太长，需要实现**上下文窗口管理**策略，如：只保留最近的 N 轮对话，或使用摘要压缩历史消息
    - 3. 既然一次对话是无状态的，如何进行多轮对话？
      - 如果你想要进行多轮对话，需要在客户端维护对话历史，并在每次请求时包含所有历史消息
        - 例如：
         第一次请求发送 
          `[{"role": "user", "content": "你好"}]`，
         第二次请求需要发送 
          `[{"role": "user", "content": "你好"}, {"role": "assistant", "content": "..."}, {"role": "user", "content": "继续"}]`