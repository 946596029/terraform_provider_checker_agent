# 2026.01.04 开发日志

## 阶段一
 * `目标`- 走通LLM调用逻辑
 * `实验环境`-
    - Windows
    - Python 3.14.x
 * `步骤`-
    - 1. 添加LLM SDK依赖(Qwen)
        ```txt
        # HTTP client libraries for API calls
        requests>=2.31.0
        httpx>=0.24.0

        # Alibaba Cloud Tongyi Qianwen
        dashscope>=1.14.0
        ```
    - 2. 
 * `问题`-
    - 1. 当使用 API 进行请求时，是一次请求是一个单独的会话？还是一次请求开启一次会话？
      * `答案`：**一次请求就是一个单独的会话**。LLM API（包括 OpenAI 和 Alibaba Cloud）都是**无状态（stateless）**的。
        这意味着：
        - 每次调用 `chat()` 方法时，都需要将完整的对话历史（messages）一起发送
        - API 服务器不会在两次请求之间保持任何状态或记忆
    - 2. LLM 的上下文限制是针对一次请求？还是针对的一次会话呢？
      * `答案`：**上下文限制是针对一次请求**的。具体来说：
        - 上下文窗口限制（context window）是指**单次 API 请求**中，输入消息（messages）和输出响应的总 token 数
        - 例如：GPT-3.5-turbo 的上下文窗口是 16K tokens，这意味着你单次请求的 messages 列表 + 生成的回复总 token 数不能超过 16K
        - 由于每次请求都是独立的，不存在跨请求的"会话级"上下文限制
        - 但需要注意的是，如果维护多轮对话，每次请求都要包含所有历史消息，所以实际上多轮对话的上下文限制就是单次请求的限制
        - 如果对话历史太长，需要实现**上下文窗口管理**策略，如：只保留最近的 N 轮对话，或使用摘要压缩历史消息
    - 3. 既然一次对话是无状态的，如何进行多轮对话？
      - 如果你想要进行多轮对话，需要在客户端维护对话历史，并在每次请求时包含所有历史消息
        - 例如：
         第一次请求发送 
          `[{"role": "user", "content": "你好"}]`，
         第二次请求需要发送 
          `[{"role": "user", "content": "你好"}, {"role": "assistant", "content": "..."}, {"role": "user", "content": "继续"}]`

---

## 阶段二
 * `目标` - 建立最基础 prompt 工程，并能够计算对上下文的占用情况
  - 1. 使用 prompt 对 LLM 进行提问，并获取回答
  - 2. 整合工具，计算每次对话对上下文的占用情况
 * `步骤`
  - 2. 需要 token 计数工具来检查文本是否超过模型上下文
  - 1. 需要 prompt 模板对内容进行封装
  - 3. 需要 会话管理器 来进行多轮对话管理
 * `问题` -
  - 1. 我如何知道我的一次会话是否超过大模型的上下文呢？
      * `答案`：可以通过以下方法检查：
        - **方法1：使用 `count_tokens()` 方法** - 在发送请求前计算 messages 的 token 数量
          ```python
          client = LLMClient(platform="alibaba")
          messages = [{"role": "user", "content": "你好"}]
          token_count = client.count_tokens(messages)
          print(f"Token count: {token_count}")
          ```
        - **方法2：使用 `check_context_limit()` 方法** - 检查是否超过上下文限制（推荐）
          ```python
          is_within_limit, info = client.check_context_limit(messages, max_tokens=1000)
          print(f"Input tokens: {info['input_tokens']}")
          print(f"Context limit: {info['context_limit']}")
          print(f"Usage: {info['usage_percentage']:.1f}%")
          print(f"Within limit: {is_within_limit}")
          ```
        - **方法3：自动检查** - `chat()` 方法默认会检查上下文限制，如果接近或超过会发出警告
          ```python
          # 默认 check_context=True，会自动检查并警告
          response = client.chat(messages=messages)
          ```
        - **常见模型的上下文窗口限制**：
          - GPT-3.5-turbo: 16,385 tokens
          - GPT-4: 8,192 tokens
          - GPT-4-turbo: 128,000 tokens
          - Qwen-turbo: 8,000 tokens
          - Qwen-plus: 32,000 tokens
        - **注意事项**：
          - Token 计数需要考虑输入消息 + 预计输出 tokens
          - 建议保留 20% 的安全余量，避免实际输出超过限制
          - 如果超过限制，需要实现上下文窗口管理策略（截断、摘要等）
  - 2. 